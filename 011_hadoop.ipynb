{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `hadoop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*soon we will need the following file, and it takes a considerable amount of time to download. start downloading it now*\n",
    "\n",
    "https://resources.oreilly.com/examples/0636920035275/raw/master/hfpd3.vmdk.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the problem(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "reach all the way back in your memory to two lectures ago, when we talked about `aws` `dynamodb`. the proposed use case for `dynamodb` was an ambiguous \"webby\" one:\n",
    "\n",
    "*we're reading and writing way too much data way too fast for our one machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this sentiment is reflective of a modern data reality that often goes by the buzziest of buzzwords:\n",
    "\n",
    "**big data**\n",
    "\n",
    "*mandatory caveat: big data $\\neq$ data science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "traditional data analyses were optimized for super-powerful single machines such as the monolithic, super-powerful `sql` servers\n",
    "\n",
    "*note: this is not to say that cluster computing didn't exist; indeed it was one of the main computational frameworks in the early days of computers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "our exponential growth in disk space and memory space per dollar spent fueled a lot of this work and innovation.\n",
    "\n",
    "basically, for a long while our ability to *compute* data grew faster than our ability to *create* or *acquire* data. in the modern world, though, that notion is absolute history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "take, for example, a relatively trivial process for modern computation: word counts for a document of several MBs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/shakespeare.txt.zip\n",
      "  inflating: /tmp/shakespeare.txt    \n",
      "-rw-r--r-- 1 zlamberty zlamberty 8.5M May 22  2009 /tmp/shakespeare.txt\n",
      "-rw-r--r-- 1 zlamberty zlamberty 2.8M Nov 30 14:31 /tmp/shakespeare.txt.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm /tmp/shakespeare.txt\n",
    "wget --quiet -O /tmp/shakespeare.txt.zip https://github.com/bbengfort/hadoop-fundamentals/raw/master/data/shakespeare.txt.zip\n",
    "unzip /tmp/shakespeare.txt.zip -d /tmp\n",
    "ls -alh /tmp/shak*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet@0\t\tHAMLET\n",
      "hamlet@8\t\n",
      "hamlet@9\t\n",
      "hamlet@10\t\tDRAMATIS PERSONAE\n",
      "hamlet@29\t\n",
      "hamlet@30\t\n",
      "hamlet@31\tCLAUDIUS\tking of Denmark. (KING CLAUDIUS:)\n",
      "hamlet@74\t\n",
      "hamlet@75\tHAMLET\tson to the late, and nephew to the present king.\n",
      "hamlet@131\t\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head /tmp/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet@0\t\tHAMLET\n",
      "hamlet@8\t\n",
      "hamlet@9\t\n",
      "hamlet@10\t\tDRAMATIS PERSONAE\n",
      "hamlet@29\t\n",
      "hamlet@30\t\n",
      "hamlet@31\tCL\n"
     ]
    }
   ],
   "source": [
    "with open('/tmp/shakespeare.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "\n",
    "print(s[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[exit]', 571),\n",
       " ('[exeunt]', 566),\n",
       " ('gloucester', 480),\n",
       " ('|', 476),\n",
       " ('falstaff', 472),\n",
       " ('hamlet', 382),\n",
       " ('othello', 292),\n",
       " ('brutus', 285),\n",
       " ('iago', 273),\n",
       " ('clown', 262)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "wordct = collections.Counter(\n",
    "    word.lower()\n",
    "    for line in s.split('\\n')\n",
    "    for word in line.strip().split('\\t')\n",
    "    if word\n",
    ")\n",
    "\n",
    "wordct.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that was easy, but it relied on some important features:\n",
    "\n",
    "1. I had enough disk space to have that 8.5MB file stored locally\n",
    "2. I had enough memory to load that 8.5MB file's contents directly into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "obviously, that isn't always the case. It's not even hard to think of counter-examples\n",
    "\n",
    "1. a larger text corpus (e.g. all of wikipedia, 10TB as of 2015, or publically available SEC EDGAR filings)\n",
    "2. any reasonably large image recognition project\n",
    "3. the logs of web traffic for any modestly sized website or service\n",
    "4. IoT information (usage records of your smartphone or headphones, e.g.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so, back to `dynamodb`. when we (theoretically) started to run into resource issues for our single-machine architecture, we decided to change the way we were doing things and start \"scaling horizontally\" -- choose an architecture and software that can spread the storage and computation burden across multiple machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "for `dynamodb` we were attempting to distribute out our database writes and reads as actions, but the test scenario I laid out above was one of\n",
    "\n",
    "+ data storage\n",
    "+ resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hadoop` is the *de facto* operating system for distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "it is a software solution that abstracts out all the \"hard stuff\" (the complicated networking and resource martialing) that needs to happen to get multiple computers on the same page, and instead provides the user (you) with a single api for\n",
    "\n",
    "+ accessing distributed files (`hdfs`)\n",
    "+ securing computational resources and memory (`yarn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `hadoop` nuts and bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's dig into the details of distributed computing a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### terminology\n",
    "\n",
    "+ **node**: a single machine (real or virtual)\n",
    "+ **cluster**: a collection of *nodes* which can communicate with eachother\n",
    "+ **master**: a *node* which can request information from or delegate tasks to other *nodes*\n",
    "+ **worker**: a *node* which merely receives, processes, and responds to requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### basic concepts\n",
    "\n",
    "in the database world we had certain requirements a `dbms` needed to meet to ensure that all clients of that database service could share those resources (the `ACID` principles)\n",
    "\n",
    "similarly, for distributed computing to be well defined and robust, we have four requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. *fault tolerance*: if one computer goes down, we're good. if it comes back, we're even gooder\n",
    "2. *recoverability*: we don't lose data when things fail\n",
    "3. *consistency*: results shouldn't depend on jobs failing or succeeding\n",
    "4. *scalability*: more data means longer time, not failure; we can increase resources if desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hadoop` addresses these requirements by making the following decisions:\n",
    "\n",
    "+ data is distribute across many nodes in the cluster; each node prefers it's local data\n",
    "+ all data is chunked into blocks (say, 128 MB) and is *replicated* (copied to other nodes)\n",
    "+ jobs (computations) are broken into tasks applied to single blocks\n",
    "+ jobs are completely unaware that they are distributed\n",
    "+ worker nodes don't care about eachother\n",
    "+ tasks are redundant and repeatable\n",
    "+ master nodes handle allocation of all resources (storage, cpus, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `hadoop` architecture\n",
    "\n",
    "`hadoop` as an operating system is basically just two pieces of software:\n",
    "\n",
    "1. `hdfs` (a program for handling distributed file storage)\n",
    "2. `yarn` (a program for handling distributed resource allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "together, these two process conspire to enforce some of those design decisions above: namely, to make sure that all data is robustly distributed and that all distributed tasks are working on local data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs` and `yarn` are the defaults and they were built to work in tandem, but either can be replaced:\n",
    "\n",
    "+ you could change stoarge methods (e.g. `hdfs` replaced by `s3`)\n",
    "+ you could change resource managers or computational layers on top of storage (e.g. `yarn` replaced by `hbase`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### a `hadoop` cluster\n",
    "\n",
    "`hadoop` is a software. the hardware is a cluster of computers. the benefit you get in using `hdfs` and `yarn` are abstracted `api`s that hide cluster administration details and tasks from you.\n",
    "\n",
    "to put it another way: `hadoop` lets someone else do the hard task of distribution so you can do what you came here to do (analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when we've talked before about databases or `aws` `REST` `api`s, we've often called them *services* and the programs we wrote to utilize those services *clients*\n",
    "\n",
    "both `hdfs` and `yarn` have several *services* that our tools (*clients*) use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs` services:\n",
    "\n",
    "+ `NameNode` (master): stores the directory tree, file metadata, file cluster locations. this is the access point for `hdfs` usage\n",
    "+ `Secondary NameNode` (master): performs housekeeping, checkpointing. not a backup `NameNode`\n",
    "+ `DataNode` (worker): local `io` for `hdfs` blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the basic interaction with `hdfs`:\n",
    "\n",
    "1. client asks `NameNode` where data lives.\n",
    "2. `NameNode` tells client\n",
    "3. client is responsible for going and getting data from `DataNode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`yarn` services:\n",
    "\n",
    "+ `ResourceManager` (master): allocates and monitor resources (memory, cores), schedules jobs\n",
    "+ `ApplicationMaster` (master): coordinates a particular app after `ResourceManager` has scheduled it\n",
    "+ `NodeManager` (worker): runs tasks and reports on task status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the basic interaction with `yarn` is very similar:\n",
    "\n",
    "1. client asks `ResourceManager` for resources\n",
    "2. `ResourceManager` assigns `ApplicationMaster` instance to manage the individual application\n",
    "3. `ApplicationMaster` submits a job to a single `NodeManager`, tracks all submitted jobs\n",
    "4. `NodeManager` executes incoming assigned tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to give you a sense of scale for a typical `hadoop` cluster\n",
    "\n",
    "+ 20 - 30 workers and one master can handle 10s of terrabytes of data in simulatneous workflows\n",
    "+ single servers (resource absolutism) is needed once you have hundreds of nodes\n",
    "+ multiple masters are needed when you start talking about thousands of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### details on `hdfs`\n",
    "\n",
    "`hdfs` is a file system on top of another filesystem. in many respects, it behaves like you're used to the `linux` filesystem behaving (with slightly different commands). there are a few nuances worth discussing, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### blocks\n",
    "\n",
    "files are blocked into large (e.g. 128MB) chunks. this means that a file larger than that will be separated up into different blocks. it's worth noting: this is effectively the *only* sense in which the block size matters\n",
    "\n",
    "a small file will not wastefully take up the remainder of the space on the OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "that's not to say there isn't a problem with small files, though -- there is. It's not wasteful disk usage, it's wasteful *resource* usage. we will discuss `mappers` and `reducers` later, but for now it suffices to say: when we distributed tasks, we already said we distribute them to blocks.\n",
    "\n",
    "if one of those blocks contains a small amount of information, that will be pretty wasteful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "better a million files of 100 MB than a billion files of 0.1 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a demo `hadoop` environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "a single `hadoop` (or, related, `aws` `emr`) environment is often a large, complicated, expensive, and unruly engineering project.\n",
    "\n",
    "to avoid the hastle of constantly building up complicated development environments, many developers will create a *virtual execution environment* in a *virtual machine*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we are going to build one such virtual environment right now using oracle's `virtualbox` and the Ubuntu 14.04 `vmdk` provided by the authors of our text book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">**walkthrough: installing `virtualbox` and a `hadoop` virtual machine**</div>\n",
    "\n",
    "1. if you didn't start the download of https://resources.oreilly.com/examples/0636920035275/raw/master/hfpd3.vmdk.gz at the beginning of class, do so now\n",
    "2. download `virtualbox` for your os and follow instructions: https://www.virtualbox.org/wiki/Downloads\n",
    "4. unzip the `vmdk` file once it is downloaded\n",
    "    1. in a terminal, `gunzip hfpd3.vmdk.gz`\n",
    "5. create the VM\n",
    "    1. open `virtualbox` and click the \"new\" button\n",
    "    2. name it whatever you want, change the type to `linux`, and make the version \"ubuntu 64-bit\"\n",
    "    3. set the memory however you want (I'll go high because yolo)\n",
    "    4. select \"Use an existing virtual hard disk file\" and navigate to the `vmdk` file\n",
    "    5. start up the VM. password is `password`\n",
    "    6. click \"Devices > Insert guest additions cd\" and the run (again, password is `password`)\n",
    "    7. restart the VM when finished (now you can resize!)\n",
    "    8. back in the `virtualbox` program, navigate to \"Settings\", and on the \"General > Advanced\" tab make \"Shared Clipboard\" \"Bidirectional\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "note: [the `cloudera` vm](https://www.cloudera.com/downloads/quickstart_vms/5-12.html) is actually pretty excellent to use and I highly recommend it for your general development and hacking.\n",
    "\n",
    "I opted for the course-specific `vmdk` so that we would avoid configuration and implementation discrepancies as much as is possible, and also because the `cloudera` download requires you provide a lot of identifying information and I am attempting to respect privacy when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\">**starting `hadoop`**</div>\n",
    "\n",
    "1. log in to your `hadoop` vm\n",
    "2. execute the following:\n",
    "\n",
    "```bash\n",
    "sudo -H -u hadoop $HADOOP_HOME/sbin/start-dfs.sh\n",
    "sudo -H -u hadoop $HADOOP_HOME/sbin/start-yarn.sh\n",
    "\n",
    "# demonstrate it worked\n",
    "hadoop fs -mkdir -p /user/student\n",
    "hadoop fs -chown student:student /user/student\n",
    "hadoop fs -ls -h /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### working with a distributed file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### basic file system operations\n",
    "\n",
    "many of the common `linux` command line file system tools are available with the same names in `hadoop`. try\n",
    "\n",
    "```bash\n",
    "hadoop fs -help\n",
    "```\n",
    "\n",
    "(note the single-dash parameters and curse the `java` gods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tired of reading those 4000 lines? try any one subcommand too:\n",
    "\n",
    "```bash\n",
    "hadoop fs -help ls\n",
    "hadoop fs -help chmod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "out on the etherwebs, you may see floating around commands such as\n",
    "\n",
    "```bash\n",
    "hdfs dfs -ls /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`hdfs dfs` is *related to* `hadoop fs`, but is not exactly the same. `hadoop fs` defaults to looking at `hdfs` files, but is actually file-system agnostic(ish), and supports local files (via the `file://` schema), `s3` files, `ftp` services, and any other people have been kind enough to implement.\n",
    "\n",
    "`hdfs dfs` *only* works with `hdfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to demonstrate how `hadoop fs` can be used with local files as well, try out\n",
    "\n",
    "```bash\n",
    "hadoop fs -ls file:///tmp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "none of this is to say *prefer* `hadoop fs` or *avoid* `hdfs dfs`. just knowing what the difference is may help you avoid some confusion when you try the subcommands or flags of one and don't experience the same result as you would with the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's prepare our `hadoop` cluster to actually do some `hadoop`-y stuff:\n",
    "\n",
    "```bash\n",
    "mkdir ~/code && cd ~/code\n",
    "git clone https://github.com/bbengfort/hadoop-fundamentals.git\n",
    "cd hadoop-fundamentals/data\n",
    "unzip shakespeare.txt.zip\n",
    "hadoop fs -copyFromLocal shakespeare.txt shakespeare.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the files we save in `hadoop` are generally enormous. it's good to know right away how to read portions of such large files:\n",
    "\n",
    "+ `hadoop fs -cat shakespeare.txt | less`\n",
    "+ `hadoop fs -cat shakespeare.txt | head` (this aborts the streaming when `head` has had enough)\n",
    "+ `hadoop fs -tail shakespeare.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### other `hdfs` interfaces\n",
    "\n",
    "finally, there are `http` integrations. in particular, check out the web interface for the `DataNode`s found at `datanode_url:50075` (for our `hadoop` vm, try `127.0.0.1:50075`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### working with distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "as we said above, `yarn` is the main resource manager and one of the main access points for computation. in the original instance of `hadoop`, however, the computational framework was a software called `mapreduce`\n",
    "\n",
    "knowing what `mapreduce` is helps illuminate the engineering paradigm at play in `hadoop` programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mapreduce`: a functional programming model\n",
    "\n",
    "`mapreduce` was [first proposed](https://research.google.com/archive/mapreduce.html) by google developers as a way of performing easily distributable computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the name comes from the two \"pieces\":\n",
    "\n",
    "+ a `map` function takes input as a series of key-value pairs (\"kvps\") and performs the same computation on each pair, generating a (possibly empty) sequence of intermediate kvps\n",
    "    + this is where analysis happens (usually)\n",
    "    + e.g. filter: take a key, check if it belongs in a list of acceptable keys, emit the kvp if yes, pass silently if no\n",
    "+ a `reduce` function takes a key and an iterator of values and process the values, usually to determine some aggregate statistic\n",
    "\n",
    "these functions out to be stateless functional programming functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `mapreduce`: implemented on a cluster\n",
    "\n",
    "the `mapreduce` framework is great for a distributed computation environment because it is assumes many of the central tenets of the distribution framework. specifically, because mappers and reducers are stateless functions, they can be executed by a worker node to independently work on any number of blocks and emit their responses back to the master node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mappers are already set: individual blocks are key-value pairs where the keys are file or line metadata and the values are the contents of the file / line. we can distribute the mapper function to any number of workers and let them process blocks at their own pace without any outside information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "reducers needs all the output values for a single key across all processed blocks, so we have to wait until all mappers are done to \"reduce\".\n",
    "\n",
    "we create as many reducers as there are output keys and distribute them among the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "because reducers expect to get the keys emitted by mappers and **all** values for those keys, we need to perform a shuffle and sort of those intermediate kvps before we can reduce. this stage is called exactly that: *shuffle and sort*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so, in the end, we have a general framework:\n",
    "\n",
    "+ input: `hdfs` kvps\n",
    "+ mapping: input kvps are processed by mappers and generate intermediate kvps\n",
    "+ shuffle and sort: take the generated key, partition the key space, and assign keys to reducers\n",
    "+ reduce: take the keys and the iterated list of values and reduce them to aggregate kvps\n",
    "\n",
    "it's kvps all the way down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### mapreduce examples\n",
    "\n",
    "we already counted words in the shakespeare corpus, in memory in plain `python`, and the pseudo-code which can fit this wordcount problem into `mapreduce` is not that different:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "def mapper(dockey, line):\n",
    "    for word in line.split():\n",
    "        emit(word, 1)\n",
    "        \n",
    "def reducer(word, values):\n",
    "    emit(word, sum(val for val in values))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### submitting a mapreduce job to `yarn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`yarn` is responsible for scheduling tasks, so if we would like to perform some task we need to give it to `yarn`.\n",
    "\n",
    "one way (and the most basic) is to create a `jar` file (compiled `java` code) and to pass that directly to `yarn` using the `hadoop jar` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "in [the github repo](https://github.com/bbengfort/hadoop-fundamentals) for the \"Data Analytics with Hadoop\" O'Reilly book, we have been provided with a couple `java` files to implement a simple `mapreduce` word count job\n",
    "\n",
    "+ [`WordCount.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/WordCount.java)\n",
    "+ [`WordMapper.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/WordMapper.java)\n",
    "+ [`SumReducer.java`](https://github.com/bbengfort/hadoop-fundamentals/blob/master/wordcount/WordCount/SumReducer.java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "let's compile and run that code on the shakespeare corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "first thing's first, let's compile our `java` code into a `jar` file\n",
    "\n",
    "```bash\n",
    "export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar\n",
    "cd ~/code/hadoop-fundamentals/wordcount/WordCount/\n",
    "hadoop com.sun.tools.javac.Main *.java\n",
    "jar cf wc.jar WordCount.class WordMapper.class SumReducer.class\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "second thing's second, let's fix a simple permission problem on our local machine and then another one on our `hdfs`.\n",
    "\n",
    "```bash\n",
    "sudo chmod g+w /var/app/hadoop/data\n",
    "sudo su hadoop\n",
    "hadoop fs -chmod g+w /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "final thing's final, we can submit the `jar` file to `yarn` by calling\n",
    "\n",
    "```bash\n",
    "hadoop jar wc.jar WordCount shakespeare.txt wordcounts\n",
    "```\n",
    "\n",
    "we can track the results of that job via a web interface at 127.0.0.1:8088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## not using `java`  with `hadoop` streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "so we were able to write `java` code to create `mapreduce` jobs. super.\n",
    "\n",
    "I mean... not knowing `java` is a bit of a problem though. not to be ungrateful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is just what we get out of the box with `hadoop` `mapreduce`.\n",
    "\n",
    "+ java api with input, output, map and reduce functions, job params exposed as *job configuration*\n",
    "+ jobs get packaged into a jar which is passed to the `ResourceManager` by the *job client*\n",
    "+ `ResourceManager` handles the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "but what if you don't want to write `java` code that implements this same workflow over and over and over again?\n",
    "\n",
    "or just don't want to write `java` code *at all*, because you already did everything you needed to do in `python`?\n",
    "\n",
    "*hadoop streaming* is here to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `hadoop` streaming\n",
    "\n",
    "hadoop streaming is a `java` util which can take any executable (in *any* language!) and use that as a mapper or reducer or combiner.\n",
    "\n",
    "really, this is just a super hacky `jar` file that is submitted in the same was as our `wc.jar` in our example above. for this `hadoop`-specific `jar` file, you pass executable scripts or commands as parameters to this `jar` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "note: the word \"streaming\" is used because the input and output method is unix streams (`stdin`, `stdout`), not in reference to streaming data.\n",
    "\n",
    "this is actually pretty cool, because we know how to access those streams:\n",
    "\n",
    "+ `python`: `sys` module\n",
    "+ `R`: `file(\"stdin\")` (I think? who even knows. does anyone?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when we develop a `mapper.py` script, know the following:\n",
    "\n",
    "+ *each* mapper launches the executable. spin-up time sucks for obvious reasons\n",
    "+ `hadoop streaming` parses input data into lines of text and pipes them through `stdin`\n",
    "+ `python` streaming script parses those lines of texts and prints (to `stdout`) kvps delimited in some way (default is `\\t`)\n",
    "+ these intermediate kvps are scooped up by `hadoop streaming` again and passed on to the reducer\n",
    "+ the mapper gets an entire block via `sys.stdin`. so it doesn't receive a *file*, or a *line number*, it receives a file handler to a block. that's important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the `reducer.py` script follows much of the same logic, but in addition:\n",
    "\n",
    "+ the reducer doesn't receive a key and an iterable, it reads shuffled and sorted kvp records (like a table) from stdin (they are in the `a\\tb` format)\n",
    "+ a single reducer task will always get *all* records for given key, but *may* get more than one key (so your reducer doesn't have a key, we need logic there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "for both files (and for any file in any language being used as a `hadoop streaming` script), the shebang (`#!`) declaration at the top of the file is important -- it tells the streaming process (a bash shell) how to execute the script (e.g. in `python`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!--div align=\"center\">***DROP joke WHERE is_bad***</div>\n",
    "<img align=\"middle\" src=\"\"></img-->\n",
    "\n",
    "# END OF LECTURE\n",
    "\n",
    "next lecture: [`spark`](012_spark.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
